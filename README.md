<div align="center">
<h1>Offical Implementation of CityGS-ùí≥</h1>

<a href="https://arxiv.org/abs/2503.23044" target="_blank" rel="noopener noreferrer">
  <img src="https://img.shields.io/badge/Paper-VGGT" alt="Paper PDF">
</a>
<a href="https://arxiv.org/abs/2503.23044"><img src="https://img.shields.io/badge/arXiv-2503.23044-b31b1b" alt="arXiv"></a>
<a href="https://lifuguan.github.io/CityGS-X/"><img src="https://img.shields.io/badge/Project_Page-green" alt="Project Page"></a>


**Northwestern Polytechnical University**; **Shanghai AI Lab**

| CityGS-X : A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction


[Yuanyuan Gao*](https://scholar.google.com/citations?hl=en&user=1zDq0q8AAAAJ), [Hao Li*](https://lifuguan.github.io/), [Jiaqi Chen*](https://github.com/chenttt2001), Zhengyu Zou, [Zhihang Zhong‚Ä†](https://zzh-tech.github.io), [Dingwen Zhang‚Ä†](https://vision-intelligence.com.cn), [Xiao Sun](https://jimmysuen.github.io), Junwei Han<br>(\* indicates equal contribution, ‚Ä† means Co-corresponding author)<br>

</div>

![Teaser image](assets/cityx_tease.jpg)

## Todo List
- [ ] Release the training & inference code.
- [ ] Release all model checkpoints.


## Installation

We tested on a server configured with Ubuntu 18.04, cuda 11.6 and gcc 9.4.0. Other similar configurations should also work, but we have not verified each one individually.

1. Clone this repo:

```
git clone https://github.com/gyy456/CityGS-X.git --recursive
cd CityGS-X
```

2. Install dependencies

```
SET DISTUTILS_USE_SDK=1 # Windows only
conda env create --file environment.yml
conda activate citygx-x
```

### Depth regularization


When training on a synthetic dataset, depth maps can be produced and they do not require further processing to be used in our method.

For real world datasets depth maps should be generated for each input images, to generate them please do the following:
1. Clone [Depth Anything v2](https://github.com/DepthAnything/Depth-Anything-V2?tab=readme-ov-file#usage):
    ```
    git clone https://github.com/DepthAnything/Depth-Anything-V2.git
    ```
2. Download weights from [Depth-Anything-V2-Large](https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true) and place it under `Depth-Anything-V2/checkpoints/`
3. Generate depth maps:
   ```
   python Depth-Anything-V2/run.py --encoder vitl --pred-only --grayscale --img-path <path to input images> --outdir <output path>
   ```
5. Generate a `depth_params.json` file using (set the depth image reslution align with the training reslution you want):
    ```
    python utils/make_depth_scale.py --base_dir <path to colmap> --depths_dir <path to generated depths>
    ```
6. use the multi-view constrains to filter the depth:
    ```
    python  multi_view_precess.py  -s  datasets/<scene_name> --resolution 4    --model_path datasets/<scene_name>/train/mask  --images train/rgbs  --pixel_thred 1
    ```

- pixel_thred: set the thred of the pixel position loss;
## Data

First, create a ```data/``` folder inside the project path by 

```
mkdir data
```

The data structure will be organised as follows:

```
data/
‚îú‚îÄ‚îÄ scene_name
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rgbs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ depths
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mask
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rgbs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000001.jpg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ sparse/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ0/
...
```





## Training

### Training multiple scenes

To train multiple scenes in parallel, we provide batch training scripts: 

 - Mill-19 and UrbanScene3D: ```train_mill19.sh```

 - MatrixCity: ```train_matrix_city.sh```

 run them with 

 ```
bash train_xxx.sh
 ```

 > Notice 1: Make sure you have enough GPU cards and memories to run these scenes at the same time.

 > Notice 2: Each process occupies many cpu cores, which may slow down the training process. Set ```torch.set_num_threads(32)``` accordingly in the ```train.py``` to alleviate it.

### Training a single scene on 4 gpu (for example)

train_mill19.sh
```
torchrun --standalone --nnodes=1 --nproc-per-node=<gpu_num>  train.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path> --iterations 100000 --images train/rgbs --single_view_weight_from_iter 10000  --depth_l1_weight_final 0.01 --depth_l1_weight_init 0.5 --dpt_loss_from_iter 10000  --multi_view_weight_from_iter 30000 --default_voxel_size 0.001 --dpt_end_iter 50_000
```

### Single gpu

```
python train.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path> --iterations 100000 --images train/rgbs --single_view_weight_from_iter 10000  --depth_l1_weight_final 0.01 --depth_l1_weight_init 0.5 --dpt_loss_from_iter 10000  --multi_view_weight_from_iter 30000 --default_voxel_size 0.001 --dpt_end_iter 50_000
```


- not_use_dpt_loss: you can jump Step2 for depth supervision;
- not_use_multi_view_loss: you can jump Step3 for depth supervision;
- not_use_single_view_loss: you can choose not use the single-view geometric loss;
- gpu_num: specify the GPU number to use ;
- bsz: set the taining batch size;
- single_view_weight_from_iter: set the start iteration of the single-view geometric loss default 10_000;
- scale_loss_from_iter:  set the start iteration of the scale loss default 0;
- dpt_loss_from_iter: set the start iteration of the depth supervision default 10_000;
- multi_view_weight_from_iter: seet the start iteration of multi-view constrains default 30_000;
- default_voxel_size: set the mean voxel size of the anchor default 0.0001;
- distributed_dataset_storage: if cpu memory is enough set it False (Load all the RGB depth and gray image on every process), if cpu memory is not enough set it Ture(Load RGB depth and gray image on one process and broadcast to other process).
- distributed_save: if Ture load the final model seperately by process, if False load the final model in one model(default)
- default_voxel_size: set the default voxel size for initialization.
- dpt_end_iter: step2 supervision depth end iteration.



## Evaluation
Except MatrixCity, evalutaion image is saved during training by default and PSNR is also calcuated.
### multi gpu

```
torchrun --standalone --nnodes=1 --nproc-per-node=<gpu_num>  render.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path>  --images train/rgbs --skip_train
```

### single gpu

```
python render.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path>  --images train/rgbs --skip_train
```

## Metric
```
python metrics.py -m output/<save_path>
```


## Mesh extract
### multi gpu
if distributed_save==Ture, you can load models with same training gpu to extract mesh

```
torchrun --standalone --nnodes=1 --nproc-per-node=<gpu_num>  train.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path>  --images train/rgbs --voxel_size 0.001 --max_depth 5 --use_depth_filter
```

if distributed_save==False, you can load the whole model on one gpu to extract the mesh
### single gpu

```
python train.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path>  --images train/rgbs --voxel_size 0.001 --max_depth 5 --use_depth_filter
```

- voxel_size: set the mesh voxel size.

## Acknowledgement
We would like to express our gratitude to the authors of the following algorithms and libraries, which have greatly inspired and supported this project:

- [Grendel-GS: On Scaling Up 3D Gaussian Splatting Training](https://github.com/nyu-systems/Grendel-GS)
- [PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction](https://zju3dv.github.io/pgsr)
- [Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians](https://city-super.github.io/octree-gs/)
- [CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes](https://dekuliutesla.github.io/CityGaussianV2)
- [Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction](https://github.com/Jixuan-Fan/Momentum-GS)



Your contributions to the open-source community have been invaluable and are deeply appreciated.

## BibTeX

```bibtex
@misc{gao2025citygsxscalablearchitectureefficient,
      title={CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction}, 
      author={Yuanyuan Gao and Hao Li and Jiaqi Chen and Zhengyu Zou and Zhihang Zhong and Dingwen Zhang and Xiao Sun and Junwei Han},
      year={2025},
      eprint={2503.23044},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.23044}, 
}
```
